1a
#lineinfile check to see if there is this line, can be used with regex
   - name: add to hostfile
      lineinfile:
         path: /etc/hosts
         line: "{{ item }}"
         state: present
         backup: yes
      with_items:
        - "{{ hostvars.db01.ansible_eth1.ipv4.address }} {{ hostvars.db01.ansible_hostname }}"
        - "{{ hostvars.db02.ansible_eth1.ipv4.address }} {{ hostvars.db02.ansible_hostname }}"
        - "{{ hostvars.quorum.ansible_eth1.ipv4.address }} {{ hostvars.quorum.ansible_hostname }}"
#chdir, create in command, shell module chdir: run at this dir, create: not run if this file exists
    - name: backup repos
      shell:
        mv `ls /etc/yum.repos.d/*.repo -I local.repo` /tmp/
      ignore_errors: yes
      args:
         creates: /etc/yum.repos.d/local.repo
#include can use include other playbooks
- include_tasks: install_with_apt.yml
  when: not install_from_source
---
b
#inventory file /etc/ansible/hosts
include host group, single host must put above group
#need to have ssh key access first to run any simple command
#basic playbook
--- YAML start, host:all host to run, tasks: task to run, name: task, yum: module
---
- hosts: drbd
  become: yes
  become_user: root
  become_method: su
  tasks:
    - name: add to hostfile
      lineinfile:
c
#inventory multi group [multi:children] [multi:vars]
#default run parallel, limit using fork -f
#run multi host, all or limit
ansible <pattern> -m <module_name> -a "<module options>"
ansible webservers -m service -a "name=httpd state=restarted"
---
2a
#idempotence: check before running, reusable
#with_items: variable list
1.1
#state: state of service
1.1
---
b
# playbook: --limit limit host, --list-hosts
ansible all -m [module] -a "[module options]" --limit "host1,host2"
ansible all -m [module] -a "[module options]" --limit 'group1'
#remote-user(-u) --become: sudo run, --become-user: run sudo with user
#--inventory(-i) specify intentory, -e --extra-vars
---
c
#App .js and npm read json to install dependencies
#register: used to decide latter whether to run
    - name: check if sdb1 exist
      stat:
        path: /dev/sdb1
      register: filesdb
#changed_when: specify the condition of change to ansible
---
3a
LAMP
#when: specify the condition to do something
    - name: Create a new ext4 primary partition
      parted:
        device: /dev/sdb
        number: 1
        part_end: 1GB
        state: present
      when: filesdb.stat.exists is defined and filesdb.stat.exists == False
#pre_task, post_task, seperate with main task
#handler: at the end, trigger by  notify of changed from a task
---
b
#file module can create file and link
#Jinja2 template can use variable in it to provide apache config
#failed_when: specify condition of fail to ansible
---
c
#ignore_errors: ignore known error
    - name: backup repos
      shell:
        mv `ls /etc/yum.repos.d/*.repo -I local.repo` /tmp/
      ignore_errors: yes
      args:
         creates: /etc/yum.repos.d/local.repo
#delegate_to: delegate task to other hosts
#local_action: task run at localhost
4a
#ansible-galaxy init role name to init tree of the role
#role help easy to see run  process, copy codes with same part to the role and replace
#default dir have default var, easily be replaced in the main playbook, var dir have higher precedence
b
#ansible also have cloud module to use cloud API (aws, digitalacean)
#dynamic inventory using result of python script to get list of servers -i file.py
#prt: use dynamic inventory to provision PaaS (like apache Solr)
solr just vagrant up
---
#using serial run (fork) to gurantee no down time when deploying, can be number of percentage
---
- name: test play
  hosts: webservers
  serial: "30%"
#ansible config show in --version and can be inited
You can generate a fully commented-out example ansible.cfg file, for example:

$ ansible-config init --disabled > ansible.cfg
https://docs.ansible.com/ansible/latest/reference_appendices/config.html
#some typing error like space in {{ }} variable could lead to weird error
4,
---
#ash: script to auto add ssh key access for ansible
ssh-keygen -f ~/.ssh/known_hosts -R 192.168.56.69
sshpass -p 123456 ssh-copy-id -o StrictHostKeyChecking=no root@192.168.56.69
#first-ansible-playbook: test chrony by dnf and service module, all host in vagrant provision
- hosts: all
  become: yes
  tasks:
  - dnf: name=chrony state=present
  - service: name=chronyd state=started enabled=yes

#drupal is a free and open-source content-management framework that can be tailored and customized to simple websites or complex web applications
---
#nodejs It is used for server-side programming, and primarily deployed for non-blocking, event-driven servers, such as traditional web sites and back-end API services, but was originally designed with real-time, push-based architectures in mind
#Adding extra package repositories (yum or apt) is one thing many admins will do before any other
work on a server to ensure that certain packages are available, or are at a later version than the ones
in the base installation
#Ansible makes things a little more robust. Even though the following is slightly more verbose, it
performs the same actions in a more structured way, which is simpler to understand, and works
with variables other nifty Ansible features we’ll discuss late
---
#
yum installs Node.js (along with all the required packages for npm, Node’s package manager) if
it’s not present, and allows the EPEL repo to be searched via the enablerepo parameter (you
could also explicitly disable a repository using disablerepo).
#dependecies like php may require latter versions
---
#Apache Solr is a fast and scalable search server optimized for full-text search, word
highlighting, faceted search, fast indexing, and more. It’s a very popular search
server, and it’s pretty easy to install and configure using Ansible
#error in the ansible is the same as bash
#galaxy-role-servers: A couple very short playbooks that demonstrate how easy it is to get new servers running leveraging the power of community Ansible Galaxy roles.
#A Memcached server provides a caching layer that can be used to store and retrieve
frequently-accessed objects in lieu of slower database storage.
5,
#prt: provision lamp stack by vagrant
vagrant up: update could be fail just rerun
#prt: provision lamp stack on aws
#prt: provision elk stack
---
#prt: provision logstash forwarder for nginx
#prt:provision gluster
#prt: provision docker container
---
#prt: deployment rail and update version
#prt: deploy mult nodejs app node
#above with proxy
---
#Capistrano’s basic style of deployment is to create dated release directories, then symlink
the current release into a stable application directory, along with resources that are
continuous among releases (like logs and uploaded files).
#Extending things a little further, many organizations use blue-green deployments.
The basic concept involves bringing up a parallel production infrastructure, then
switching over to it. The cutover may take only a few milliseconds and no active
production infrastructure is ever offline during the deployment process
#Another important aspect of a successful deployment is communication. If you’re
running playbooks as part of a CI/CD process, or in some other automated fashion,
use one of the many built-in Ansible notification modules to share the deployment’s
progress via chat, email, or even text-to-speech on your Mac with the osx_say
module! Ansible includes easy-to-use notification modules for:
6,
#prt: provision ansible tower
#prt: provision jenkins server
#prt: Create a Jenkins job to run an Ansible Playbook
---
#Unit testing, when applied to applications, is testing of the smallest units of code
(usually functions or class methods). In Ansible, unit testing would typically apply
to individual playbooks. You could run individual playbooks in an isolated environment, but it’s often not worth the effort. What is worth your effort is at least checking
the playbook syntax, to make sure you didn’t just commit a YAML file that will break
an entire deployment because of a missing quotation mark, or a whitespace issue!
#Integration testing, which is definitely more valuable when it comes to Ansible, is
the testing of small groupings of individual units of code, to make sure they work
correctly together. Breaking your infrastructure definition into many task-specific
roles and playbooks allows you to do this; if you’ve structured your playbooks so
they have no or limited dependencies, you could 
#Functional testing involves the whole shebang. Basically, you set up a complete
infrastructure environment, and then run tests against it to make sure everything was
successfully installed, deployed, and configured. Ansible’s own reporting is helpful
in this kind of testing, and there are external tools available to test infrastructure
even more deeply
---
#Ansible has a debug module, which prints variables or messages
during playbook execution.
#Both fail and assert, when triggered, will abort the playbook run, and the only
difference is in the simplicity of their usage. 
#Two checks you should include in an automated playbook testing workflow are -
-syntax-check (which checks the playbook syntax to find quoting, formatting, or
whitespace errors) and --check (which will run your entire playbook in check mode
---
#When using --check mode, certain tasks may need to be forced to run to ensure the
playbook completes successfully: (e.g. a command task that registers variables used in
later tasks). You can set check_mode: no to do this
#Automated testing using a continuous integration tool like Travis CI (which is free
for public projects and integrated very well with GitHub) allows you to run tests
against Ansible playbooks or roles you have hosted on GitHub with every commit.
There are four main things to test when building and maintaining Ansible playbooks
or roles:
1. The playbook or role’s syntax (are all the .yml files formatted correctly?).
2. Whether the playbook or role will run through all the included tasks without
failing.
3. The playbook or role’s idempotence (if run again, it should not make any
changes!).
4. The playbook or role’s success (does the role do what it should be doing?):
7,
--
#prt: Testing on multiple OSes with Docker
#Serverspec167 is a tool to help automate server tests using RSpec tests, which use a
Ruby-like DSL to ensure your server configuration matches your expectations. In a
sense, it’s another way of building well-tested infrastructure.
Practical:i
#build lamp on aws
---
- hosts: drbd
  become: yes
  become_user: root
  become_method: su
  tasks:
    - name: add to hostfile
      lineinfile:
         path: /etc/hosts
         line: "{{ item }}"
         state: present
         backup: yes
      with_items:
        - "{{ hostvars.db01.ansible_eth1.ipv4.address }} {{ hostvars.db01.ansible_hostname }}"
        - "{{ hostvars.db02.ansible_eth1.ipv4.address }} {{ hostvars.db02.ansible_hostname }}"
        - "{{ hostvars.quorum.ansible_eth1.ipv4.address }} {{ hostvars.quorum.ansible_hostname }}"

    - name: disable selinux
      selinux:
        state: disabled


    - name: backup repos
      shell:
        mv `ls /etc/yum.repos.d/*.repo -I local.repo` /tmp/
      ignore_errors: yes
      args:
         creates: /etc/yum.repos.d/local.repo
    - name: create file local repos
      file:
        path: /etc/yum.repos.d/local.repo
        state: touch

    - name: add new local repos
      lineinfile:
         path: /etc/yum.repos.d/local.repo
         line: "{{ item }}"
         state: present
         backup: yes
      with_items:
        - "[CentOS79] "
        - "name=CentOS x86_64 "
        - "baseurl=file:///media "
        - "gpgcheck=0 "
        - "enabled=1  "


    - name: Mount volume
      mount:
        src: /dev/cdrom
        path: /media
        state: mounted
        fstype: iso9660
    - name: Install the 'Development tools' package group
      yum:
        name: "@Development tools"
        state: present
    - name: Install lvm2
      yum:
        name: lvm2
        state: present

    - name: check if sdb1 exist
      stat:
        path: /dev/sdb1
      register: filesdb

    - name: Create a new ext4 primary partition
      parted:
        device: /dev/sdb
        number: 1
        part_end: 1GB
        state: present
      when: filesdb.stat.exists is defined and filesdb.stat.exists == False
    - name: format
      filesystem:
        fstype: ext4
        dev: /dev/sdb1
      when: filesdb.stat.exists is defined and filesdb.stat.exists == False
    - name: task for creating volume group
      lvg:
        vg: vg00
        pvs: /dev/sdb1
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
    - name: task for creating volume group for target
      lvg:
        vg: vg_iscsi
        pvs: /dev/sdb1
      when: ansible_hostname == "quorum"

    - name: create lvm for target
      lvol:
        vg: vg_iscsi
        size: 100%FREE
        lv: lv_iscsi
        shrink: no
      when: ansible_hostname == "quorum"

    - name: create lvm
      lvol:
        vg: vg00
        size: 100%FREE
        lv: drbd-r0
        shrink: no
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
    - name: install target
      yum:
        name: targetcli
        state: present
      when: ansible_hostname == "quorum"

    - name: json config change
      shell: |
       targetctl restore /root/scripts/target/saveconfig.json
       targetctl save /etc/target/saveconfig.json
      when: ansible_hostname == "quorum"
      args:
        creates: /etc/target/saveconfig.json
    - name: start target
      service:
        name: target
        state: started
        enabled: yes
      when: ansible_hostname == "quorum"

    - name: install pcs
      yum:
        name:
          - pacemaker
          - pcs
          - resource-agents
          - sbd
          - fence-agents-sbd
          - fence-agents-kdump
          - iscsi-initiator-utils
    - name: enable pcs
      systemd:
        name: "{{ item }}"
        state: started
        enabled: yes
      with_items:
          - pcsd
          - corosync
          - pacemaker
          - iscsid
      ignore_errors: True
      when: ansible_hostname == "db01" or ansible_hostname == "db02"

    - name: initialize cluster
      shell: |
           echo -e "123\n123" | passwd hacluster
           echo -e "123\n"|pcs cluster auth db01 db02 -u hacluster
           pcs cluster setup --name mycluster db01 db02
           pcs cluster start --all
           pcs cluster enable --all
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
      args:
        creates: /etc/corosync/corosync.conf


    - name: install sbd

      shell: |
          echo "InitiatorName=iqn.2022-06.gdc.managed.service:{{ ansible_hostname }}">/etc/iscsi/initiatorname.iscsi
          systemctl restart iscsid
          systemctl enable iscsid
          iscsiadm -m discovery -t st -p 192.168.56.83
          iscsiadm -m node -T iqn.2022-06.gdc.managed.service:disk1 -p 192.168.56.83 -l
          iscsiadm -m node -T iqn.2022-06.gdc.managed.service:disk1 -p 192.168.56.83 -u
          iscsiadm -m node -p 192.168.56.83 --op=update --name=node.startup --value=automatic
          iscsiadm -m node -T iqn.2022-06.gdc.managed.service:disk1 -p 192.168.56.83 -l
          modprobe -v softdog
          echo softdog > /etc/modules-load.d/watchdog.conf
          systemctl restart systemd-modules-load
          sleep 1m
          scsi_id=`ls -al /dev/disk/by-id/|grep scsi|awk '{print $9}'`
          sbd -d /dev/disk/by-id/$scsi_id -1 5 -4 10 create
          sbd -d "/dev/disk/by-id/$scsi_id" dump
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
      args:
        creates: /etc/modules-load.d/watchdog.conf
    - name: pacemaker sbd config
      shell: |
         pcs cluster stop --all
         sleep 1
         pcs stonith sbd enable
         sleep 1
         pcs cluster start --all
         sleep 1
         pcs cluster cib /root/mycluster
         pcs -f /root/mycluster stonith create kdump fence_kdump pcmk_host_check=static-list pcmk_host_list="db01, db02" pcmk_monitor_action=metadata pcmk_off_retries=1 pcmk_reboot_action=off pcmk_status_action=metadata pcmk_off_timeout=120 timeout=120
         pcs -f /root/mycluster stonith level add 1 db01 kdump
         pcs -f /root/mycluster stonith level add 1 db02 kdump
         pcs -f /root/mycluster stonith create mysbd01 fence_sbd pcmk_host_list="db01, db02" devices="/dev/disk/by-id/scsi-3600140579e9ac27ea2a4290a9c2cdc94" pcmk_delay_base=5 pcmk_delay_max=15 power_timeout=25
         pcs -f /root/mycluster stonith level add 2 db01 mysbd01
         pcs -f /root/mycluster stonith level add 2 db02 mysbd01
         pcs -f /root/mycluster stonith enable --watchdog=/dev/watchdog --device=/dev/disk/by-id/scsi-3600140579e9ac27ea2a4290a9c2cdc94 mysbd01
      when: ansible_hostname == "db01"
      args:
        creates: /root/mycluster
    - name: onstall drbd
      yum:
         name:
           - /root/scripts/packages/drbd/drbd90-utils-9.20.2-1.el7.elrepo.x86_64.rpm
           - /root/scripts/packages/drbd/kmod-drbd90-9.1.6-1.el7_9.elrepo.x86_64.rpm
         state: present
      when: ansible_hostname == "db01" or ansible_hostname == "db02"

    - name: modprobe
      shell: |
         modprobe drbd
         echo drbd > /etc/modules-load.d/drbd.conf
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
      args:
        creates: /etc/modules-load.d/drbd.conf
    - name: create file drbd config
      file:
        path: /etc/drbd.d/drbd_r0.res
        state: touch
      when: ansible_hostname == "db01" or ansible_hostname == "db02"

    - name: config drbd
      shell: |
         echo "resource r0 {                                   ">     /etc/drbd.d/drbd_r0.res
         echo "	protocol C;                                   ">>     /etc/drbd.d/drbd_r0.res
         echo "	startup {                                     ">>     /etc/drbd.d/drbd_r0.res
         echo "		degr-wfc-timeout 60;                      ">>     /etc/drbd.d/drbd_r0.res
         echo "		outdated-wfc-timeout 30;                  ">>     /etc/drbd.d/drbd_r0.res
         echo "		wfc-timeout 20;                           ">>     /etc/drbd.d/drbd_r0.res
         echo "	}                                             ">>     /etc/drbd.d/drbd_r0.res
         echo "	disk {                                        ">>     /etc/drbd.d/drbd_r0.res
         echo "		on-io-error detach;                       ">>     /etc/drbd.d/drbd_r0.res
         echo "	}                                             ">>     /etc/drbd.d/drbd_r0.res
         echo "	net {                                         ">>     /etc/drbd.d/drbd_r0.res
         echo "		cram-hmac-alg sha1;                       ">>     /etc/drbd.d/drbd_r0.res
         echo "		shared-secret \"Daveisc00l123313\";       ">>     /etc/drbd.d/drbd_r0.res
         echo "	}                                             ">>     /etc/drbd.d/drbd_r0.res
         echo "	volume 0 {                                    ">>     /etc/drbd.d/drbd_r0.res
         echo "			device /dev/drbd0;                    ">>     /etc/drbd.d/drbd_r0.res
         echo "			disk /dev/mapper/vg00-drbd--r0;       ">>     /etc/drbd.d/drbd_r0.res
         echo "			meta-disk internal;                   ">>     /etc/drbd.d/drbd_r0.res
         echo "	}                                             ">>     /etc/drbd.d/drbd_r0.res
         echo "	on db01 {                               ">>     /etc/drbd.d/drbd_r0.res
         echo "		address 192.168.56.81:7789;              	  ">>     /etc/drbd.d/drbd_r0.res
         echo "	}                                             ">>     /etc/drbd.d/drbd_r0.res
         echo "	on db02 {                               ">>     /etc/drbd.d/drbd_r0.res
         echo "	        address 192.168.56.82:7789;              	  ">>     /etc/drbd.d/drbd_r0.res
         echo "	}                                             ">>     /etc/drbd.d/drbd_r0.res
         echo "}                                               ">>     /etc/drbd.d/drbd_r0.res
         drbdadm create-md r0 --force
         drbdadm up r0
         drbdadm primary r0 --force
         drbdadm -- --overwrite-data-of-peer primary all
         drbdadm outdate r0
         mkfs.ext4 /dev/drbd0
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
    - name: add resource drbd
      shell: |
         pcs -f /root/mycluster resource create r0 ocf:linbit:drbd drbd_resource=r0 op monitor interval=10s
         pcs -f /root/mycluster resource master r0-clone r0 master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true
         pcs -f /root/mycluster resource create drbd-fs-pgdata Filesystem device="/dev/drbd0" directory="/pgdata" fstype="ext4"
         pcs -f /root/mycluster constraint colocation add drbd-fs-pgdata with r0-clone INFINITY with-rsc-role=Master
         pcs -f /root/mycluster resource create vip1 ocf:heartbeat:IPaddr2 ip=192.168.56.100 cidr_netmask=24 op monitor interval=10s
         pcs -f /root/mycluster constraint colocation add vip1 with drbd-fs-pgdata INFINITY
         pcs -f /root/mycluster constraint order drbd-fs-pgdata then vip1
         pcs -f /root/mycluster resource show
         pcs -f /root/mycluster constraint
         pcs cluster cib-push /root/mycluster --config
      when: ansible_hostname == "db01"

    - name: install postgres
      yum:
         name:
           - /root/scripts/packages/postgres14/postgresql14-server-14.3-1PGDG.rhel7.x86_64.rpm
           - /root/scripts/packages/postgres14/postgresql14-contrib-14.3-1PGDG.rhel7.x86_64.rpm
           - /root/scripts/packages/postgres14/postgresql14-libs-14.3-1PGDG.rhel7.x86_64.rpm
           - /root/scripts/packages/postgres14/postgresql14-14.3-1PGDG.rhel7.x86_64.rpm
         state: present
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
      tags:
           - postgres

    - name: "Find out if PostgreSQL is initialized"
      stat:
        path: /var/lib/pgsql/14/data/postgresql.conf
      register: postgres_data
      tags:
           - postgres
    - name: "Initialize PostgreSQL"
      shell: |
          /usr/pgsql-14/bin/postgresql-14-setup initdb
      when:
           - postgres_data.stat.exists == False
           - ansible_hostname == "db01" or ansible_hostname == "db02"
      tags:
           - postgres
    - name: create pgdata folder
      file:
         path: /pgdata
         owner: postgres
         group: postgres
         state: directory
         mode: 0770
         recurse: yes
      when: ansible_hostname == "db01" or ansible_hostname == "db02"

    - name: change configuration of postgres
      shell: |
         rsync -av /var/lib/pgsql/14/data /pgdata/
         sed -i "s/#data_directory = 'ConfigDir'/data_directory = '\/pgdata\/data'/g" /pgdata/data/postgresql.conf
         sed -i "s/Environment=PGDATA=\/var\/lib\/pgsql\/14\/data/Environment=PGDATA=\/pgdata\/data/g" /lib/systemd/system/postgresql-14.service
         systemctl daemon-reload
         systemctl start postgresql-14.service
         systemctl stop postgresql-14.service
      tags:
           - postgres
      when: ansible_hostname == "db01" or ansible_hostname == "db02"
    - name: add to drbd
      shell: |
         pcs -f /root/mycluster resource create pgsql ocf:heartbeat:pgsql pgctl=/usr/pgsql-14/bin/pg_ctl psql=/usr/pgsql-14/bin/psql pgdata=/pgdata/data op monitor timeout=30s interval=30s
         pcs -f /root/mycluster constraint colocation add pgsql with vip1 INFINITY
         pcs -f /root/mycluster constraint order vip1 then pgsql
         pcs -f /root/mycluster constraint order promote r0-clone then start drbd-fs-pgdata
         pcs resource cleanup
         pcs cluster cib-push /root/mycluster --config
      when: ansible_hostname == "db01"
      tags:
           - postgres

